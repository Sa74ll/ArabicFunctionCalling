{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üá∏üá¶ Arabic FunctionGemma Fine-tuning\n",
                "\n",
                "**Dataset:** Sa74ll/arabic-mobile-actions (45,729 samples)\n",
                "**Model:** google/functiongemma-270m-it\n",
                "**GPU:** A100 (40GB)\n",
                "\n",
                "---\n",
                "\n",
                "## Key Features:\n",
                "- ‚úÖ Checkpoint every 500 steps to Google Drive\n",
                "- ‚úÖ Auto-resume from last checkpoint if Colab crashes\n",
                "- ‚úÖ Optimized for A100 GPU\n",
                "- ‚úÖ WandB logging"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Mount Google Drive (IMPORTANT!)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import drive\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "# Create checkpoint directory\n",
                "import os\n",
                "CHECKPOINT_DIR = '/content/drive/MyDrive/arabic_functiongemma_checkpoints'\n",
                "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
                "print(f\"‚úÖ Checkpoints will be saved to: {CHECKPOINT_DIR}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Install Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%capture\n",
                "!pip install unsloth\n",
                "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n",
                "!pip install wandb"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Load Model with LoRA"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from unsloth import FastModel\n",
                "import torch\n",
                "\n",
                "# Check GPU\n",
                "print(f\"üñ•Ô∏è GPU: {torch.cuda.get_device_name(0)}\")\n",
                "print(f\"üíæ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
                "\n",
                "# Load FunctionGemma\n",
                "model, tokenizer = FastModel.from_pretrained(\n",
                "    model_name=\"google/functiongemma-270m-it\",\n",
                "    max_seq_length=2048,\n",
                "    load_in_4bit=True,  # Memory efficient\n",
                "    dtype=None,\n",
                ")\n",
                "\n",
                "# Add LoRA adapters\n",
                "model = FastModel.get_peft_model(\n",
                "    model,\n",
                "    r=32,                  # LoRA rank\n",
                "    lora_alpha=64,         # 2x rule\n",
                "    lora_dropout=0.05,     # Slight regularization\n",
                "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
                "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
                "    use_gradient_checkpointing=\"unsloth\",\n",
                "    random_state=3407,\n",
                ")\n",
                "\n",
                "print(\"‚úÖ Model loaded with LoRA!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Load Arabic Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from datasets import load_dataset\n",
                "\n",
                "# Load YOUR Arabic dataset\n",
                "dataset = load_dataset(\"Sa74ll/arabic-mobile-actions\", split=\"train\")\n",
                "print(f\"üìä Dataset size: {len(dataset):,} samples\")\n",
                "\n",
                "# Check a sample\n",
                "print(f\"\\nüìù Sample query: {dataset[0]['messages'][1]['content'][:100]}...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Process Dataset for Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def process_dataset(row, tokenizer):\n",
                "    \"\"\"Convert messages + tools to training text format.\"\"\"\n",
                "    text = tokenizer.apply_chat_template(\n",
                "        row[\"messages\"],\n",
                "        tools=row[\"tools\"],\n",
                "        tokenize=False,\n",
                "        add_generation_prompt=False,\n",
                "    )\n",
                "    return {\"text\": text}\n",
                "\n",
                "# Process all samples\n",
                "dataset = dataset.map(\n",
                "    process_dataset, \n",
                "    fn_kwargs={\"tokenizer\": tokenizer},\n",
                "    num_proc=4  # Parallel processing\n",
                ")\n",
                "\n",
                "print(\"‚úÖ Dataset processed!\")\n",
                "print(f\"üìè Sample text length: {len(dataset[0]['text'])} chars\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Setup WandB (Optional but Recommended)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import wandb\n",
                "\n",
                "# Login to WandB (run once, then comment out)\n",
                "# !wandb login\n",
                "\n",
                "wandb.init(\n",
                "    project=\"Arabic-FunctionGemma\",\n",
                "    name=\"arabic-ft-v1\",\n",
                "    config={\n",
                "        \"model\": \"functiongemma-270m\",\n",
                "        \"dataset\": \"Sa74ll/arabic-mobile-actions\",\n",
                "        \"dataset_size\": len(dataset),\n",
                "        \"lora_r\": 32,\n",
                "        \"learning_rate\": 1e-4,\n",
                "        \"batch_size\": 8,\n",
                "        \"gradient_accumulation\": 4,\n",
                "        \"effective_batch_size\": 32,\n",
                "    },\n",
                "    tags=[\"arabic\", \"function-calling\", \"gemma\"],\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Configure Training (A100 Optimized)\n",
                "\n",
                "### Key Settings:\n",
                "- **Batch size 8** (A100 can handle it)\n",
                "- **Gradient accumulation 4** ‚Üí Effective batch = 32\n",
                "- **Save every 500 steps** to Google Drive\n",
                "- **Auto-resume** from last checkpoint"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from trl import SFTTrainer, SFTConfig\n",
                "\n",
                "# Split for evaluation\n",
                "split_dataset = dataset.train_test_split(test_size=500, shuffle=True, seed=3407)\n",
                "print(f\"üìä Train: {len(split_dataset['train']):,} | Eval: {len(split_dataset['test']):,}\")\n",
                "\n",
                "# === TRAINING CONFIG (A100 OPTIMIZED) ===\n",
                "trainer = SFTTrainer(\n",
                "    model=model,\n",
                "    tokenizer=tokenizer,\n",
                "    train_dataset=split_dataset[\"train\"],\n",
                "    eval_dataset=split_dataset[\"test\"],\n",
                "    args=SFTConfig(\n",
                "        # === Core Training ===\n",
                "        dataset_text_field=\"text\",\n",
                "        max_steps=6000,                      # ~4 epochs\n",
                "        per_device_train_batch_size=8,       # A100 can handle 8\n",
                "        gradient_accumulation_steps=4,       # Effective batch = 32\n",
                "        \n",
                "        # === Learning Rate ===\n",
                "        learning_rate=1e-4,                  # Conservative for Arabic\n",
                "        warmup_steps=200,                    # Warm start\n",
                "        lr_scheduler_type=\"cosine\",          # Smooth decay\n",
                "        weight_decay=0.01,\n",
                "        \n",
                "        # === Checkpointing (CRITICAL!) ===\n",
                "        save_strategy=\"steps\",\n",
                "        save_steps=500,                      # Save every 500 steps\n",
                "        save_total_limit=5,                  # Keep last 5 checkpoints\n",
                "        output_dir=CHECKPOINT_DIR,           # Save to Google Drive!\n",
                "        \n",
                "        # === Evaluation ===\n",
                "        eval_strategy=\"steps\",\n",
                "        eval_steps=500,                      # Evaluate every 500 steps\n",
                "        \n",
                "        # === Logging ===\n",
                "        logging_steps=25,\n",
                "        logging_first_step=True,\n",
                "        report_to=\"wandb\",\n",
                "        run_name=\"arabic-functiongemma-v1\",\n",
                "        \n",
                "        # === Optimizer ===\n",
                "        optim=\"adamw_8bit\",                  # Memory efficient\n",
                "        \n",
                "        # === Misc ===\n",
                "        seed=3407,\n",
                "        bf16=True,                           # A100 supports bf16\n",
                "    ),\n",
                ")\n",
                "\n",
                "print(\"‚úÖ Trainer configured!\")\n",
                "print(f\"üìÅ Checkpoints: {CHECKPOINT_DIR}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Train Only on Model Responses\n",
                "\n",
                "This is crucial! We only want the model to learn the **response** part, not the user query."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from unsloth.chat_templates import train_on_responses_only\n",
                "\n",
                "trainer = train_on_responses_only(\n",
                "    trainer,\n",
                "    instruction_part=\"<start_of_turn>user\\n\",\n",
                "    response_part=\"<start_of_turn>model\\n\",\n",
                ")\n",
                "\n",
                "print(\"‚úÖ Set to train on responses only!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Check Memory Before Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "gpu_stats = torch.cuda.get_device_properties(0)\n",
                "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
                "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
                "\n",
                "print(f\"üñ•Ô∏è GPU: {gpu_stats.name}\")\n",
                "print(f\"üíæ Max Memory: {max_memory} GB\")\n",
                "print(f\"üìä Currently Reserved: {start_gpu_memory} GB\")\n",
                "print(f\"‚úÖ Available for Training: {max_memory - start_gpu_memory:.1f} GB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. üöÄ Start Training!\n",
                "\n",
                "### Important Notes:\n",
                "- Training will take ~1-1.5 hours on A100\n",
                "- Checkpoints save to Drive every 500 steps (~8 minutes)\n",
                "- If Colab crashes, just re-run from Cell 1 - it will auto-resume!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "# Check for existing checkpoints to resume from\n",
                "checkpoints = [d for d in os.listdir(CHECKPOINT_DIR) if d.startswith('checkpoint-')] if os.path.exists(CHECKPOINT_DIR) else []\n",
                "\n",
                "if checkpoints:\n",
                "    latest = sorted(checkpoints, key=lambda x: int(x.split('-')[1]))[-1]\n",
                "    resume_path = os.path.join(CHECKPOINT_DIR, latest)\n",
                "    print(f\"üîÑ Resuming from: {resume_path}\")\n",
                "else:\n",
                "    resume_path = None\n",
                "    print(\"üÜï Starting fresh training...\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*50)\n",
                "print(\"üöÄ STARTING TRAINING\")\n",
                "print(\"=\"*50)\n",
                "\n",
                "trainer_stats = trainer.train(resume_from_checkpoint=resume_path)\n",
                "\n",
                "print(\"\\n\" + \"=\"*50)\n",
                "print(\"‚úÖ TRAINING COMPLETE!\")\n",
                "print(\"=\"*50)\n",
                "\n",
                "wandb.finish()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. Training Stats"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
                "used_memory_for_training = round(used_memory - start_gpu_memory, 3)\n",
                "\n",
                "print(f\"‚è±Ô∏è Training Time: {trainer_stats.metrics['train_runtime']:.0f} seconds\")\n",
                "print(f\"‚è±Ô∏è Training Time: {trainer_stats.metrics['train_runtime']/60:.1f} minutes\")\n",
                "print(f\"üíæ Peak Memory: {used_memory} GB\")\n",
                "print(f\"üíæ Memory for Training: {used_memory_for_training} GB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 12. Test the Fine-tuned Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test queries in Arabic\n",
                "test_queries = [\n",
                "    \"ŸÖÿß ŸáŸà ÿßŸÑÿ∑ŸÇÿ≥ ŸÅŸä ÿßŸÑÿ±Ÿäÿßÿ∂ÿü\",\n",
                "    \"ÿßÿ≠ÿ¨ÿ≤ ŸÑŸä ŸÖŸàÿπÿØ ŸÖÿπ ÿØŸÉÿ™Ÿàÿ± ÿ£ÿ≥ŸÜÿßŸÜ\",\n",
                "    \"ÿπÿßŸäÿ≤ ÿ£ÿπÿ±ŸÅ ŸÖŸàÿßÿπŸäÿØ ÿßŸÑÿµŸÑÿßÿ© ŸÅŸä ÿßŸÑŸÇÿßŸáÿ±ÿ©\",\n",
                "]\n",
                "\n",
                "# Sample tools for testing\n",
                "test_tools = [\n",
                "    {\n",
                "        \"type\": \"function\",\n",
                "        \"function\": {\n",
                "            \"name\": \"get_weather\",\n",
                "            \"description\": \"Get weather for a city\",\n",
                "            \"parameters\": {\n",
                "                \"type\": \"object\",\n",
                "                \"properties\": {\"city\": {\"type\": \"string\"}},\n",
                "                \"required\": [\"city\"]\n",
                "            }\n",
                "        }\n",
                "    }\n",
                "]\n",
                "\n",
                "from transformers import TextStreamer\n",
                "\n",
                "for query in test_queries:\n",
                "    print(f\"\\n{'='*50}\")\n",
                "    print(f\"üìù Query: {query}\")\n",
                "    print(f\"{'='*50}\")\n",
                "    \n",
                "    messages = [\n",
                "        {\"role\": \"developer\", \"content\": \"You are a helpful assistant that can use tools.\"},\n",
                "        {\"role\": \"user\", \"content\": query}\n",
                "    ]\n",
                "    \n",
                "    text = tokenizer.apply_chat_template(\n",
                "        messages,\n",
                "        tools=test_tools,\n",
                "        tokenize=False,\n",
                "        add_generation_prompt=True,\n",
                "    ).removeprefix('<bos>')\n",
                "    \n",
                "    _ = model.generate(\n",
                "        **tokenizer(text, return_tensors=\"pt\").to(\"cuda\"),\n",
                "        max_new_tokens=256,\n",
                "        streamer=TextStreamer(tokenizer, skip_prompt=True),\n",
                "        temperature=0.1,\n",
                "        do_sample=True,\n",
                "    )"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 13. Save Final Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save to Drive\n",
                "FINAL_MODEL_PATH = \"/content/drive/MyDrive/arabic_functiongemma_final\"\n",
                "\n",
                "# Save LoRA adapters\n",
                "model.save_pretrained(FINAL_MODEL_PATH)\n",
                "tokenizer.save_pretrained(FINAL_MODEL_PATH)\n",
                "\n",
                "print(f\"‚úÖ Model saved to: {FINAL_MODEL_PATH}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 14. (Optional) Push to HuggingFace Hub"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Uncomment and run to push to HuggingFace\n",
                "\n",
                "# HF_TOKEN = \"your_token_here\"\n",
                "# HF_REPO = \"Sa74ll/arabic-functiongemma-270m\"\n",
                "\n",
                "# model.push_to_hub(HF_REPO, token=HF_TOKEN)\n",
                "# tokenizer.push_to_hub(HF_REPO, token=HF_TOKEN)\n",
                "\n",
                "# print(f\"‚úÖ Pushed to: https://huggingface.co/{HF_REPO}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 15. (Optional) Merge & Export to GGUF"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Uncomment to merge LoRA and save as full model\n",
                "\n",
                "# model.save_pretrained_merged(\n",
                "#     \"/content/drive/MyDrive/arabic_functiongemma_merged\",\n",
                "#     tokenizer,\n",
                "#     save_method=\"merged_16bit\"\n",
                "# )\n",
                "\n",
                "# For GGUF export (for llama.cpp):\n",
                "# model.save_pretrained_gguf(\n",
                "#     \"/content/drive/MyDrive/arabic_functiongemma_gguf\",\n",
                "#     tokenizer,\n",
                "#     quantization_method=\"Q8_0\"\n",
                "# )"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        },
        "accelerator": "GPU",
        "gpuClass": "premium"
    },
    "nbformat": 4,
    "nbformat_minor": 4
}